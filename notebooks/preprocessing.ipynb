{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8fc12c-c360-4bbb-a731-2d3da8c32d44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00fd00bb65b4c50bff5be5316c31125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5684e19ace993ce9\n",
      "Found cached dataset imagefolder (C:/Users/Владимир/.cache/huggingface/datasets/imagefolder/default-5684e19ace993ce9/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, Dataset, Features, Array3D,Array2D\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir='C:\\\\research\\\\data\\\\fashion_images_structured', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af95c2e-a26a-42fd-b9da-0b420dd1e611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def split_dataset(\n",
    "    dataset: Dataset,\n",
    "    val_size: float=0.2,\n",
    "    test_size: float=0.1\n",
    ") -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Returns a tuple with three random train, validation and test subsets by splitting the passed dataset.\n",
    "    Size of the validation and test sets defined as a fraction of 1 with the `val_size` and `test_size` arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Splitting dataset into train, validation and test sets...\")\n",
    "\n",
    "    # Split dataset into train and (val + test) sets\n",
    "    split_size = round(val_size + test_size, 3)\n",
    "    dataset = dataset.train_test_split(shuffle=True, test_size=split_size)\n",
    "\n",
    "    # Split (val + test) into val and test sets\n",
    "    split_ratio = round(test_size / (test_size + val_size), 3)\n",
    "    val_test_sets = dataset['test'].train_test_split(shuffle=True, test_size=split_ratio)\n",
    "\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    val_dataset = val_test_sets[\"train\"]\n",
    "    test_dataset = val_test_sets[\"test\"]\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f4aac6-b881-4f0b-bfc5-5b6a172b6266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "test_size = 0.1\n",
    "model_name = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef53a46c-ee1f-44fa-bb5b-d24278b4746a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train, validation and test sets...\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4076672-c7a1-48f8-a57e-f9759ca3a209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, ViTFeatureExtractor, ViTForImageClassification, Trainer, TrainingArguments, default_data_collator\n",
    "\n",
    "\n",
    "def process_examples(examples, image_processor):\n",
    "    \"\"\"Processor helper function. Used to process batches of images using the\n",
    "    passed image_processor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples\n",
    "        A batch of image examples.\n",
    "\n",
    "    image_processor\n",
    "        A HuggingFace image processor for the selected model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    examples\n",
    "        A batch of processed image examples.\n",
    "    \"\"\"\n",
    "    # Get batch of images\n",
    "    images = examples['image']\n",
    "    # images = [image.convert(\"RGB\").resize((32,32)) for image in examples[\"image\"]]\n",
    "\n",
    "    # Preprocess\n",
    "    inputs = image_processor(images=images)\n",
    "    # Add pixel_values\n",
    "    examples['pixel_values'] = inputs['pixel_values']\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def apply_processing(\n",
    "    model_name: str,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    test_dataset: Dataset\n",
    ") -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Apply model's image AutoProcessor to transform train, validation and test subsets.\n",
    "    Returns train, validation and test datasets with `pixel_values` in torch tensor type.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extend the features\n",
    "    features = Features({\n",
    "        **train_dataset.features,\n",
    "        'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    })\n",
    "\n",
    "    # Instantiate image_processor\n",
    "    image_processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # Preprocess images\n",
    "    train_dataset = train_dataset.map(process_examples, batched=True, features=features, fn_kwargs={\"image_processor\": image_processor}, batch_size=2500)\n",
    "    val_dataset = val_dataset.map(process_examples, batched=True, features=features, fn_kwargs={\"image_processor\": image_processor}, batch_size=2500)\n",
    "    test_dataset = test_dataset.map(process_examples, batched=True, features=features, fn_kwargs={\"image_processor\": image_processor}, batch_size=2500)\n",
    "\n",
    "    # Set to torch format for training\n",
    "    train_dataset.set_format('torch', columns=['pixel_values', 'label'])\n",
    "    val_dataset.set_format('torch', columns=['pixel_values', 'label'])\n",
    "    test_dataset.set_format('torch', columns=['pixel_values', 'label'])\n",
    "\n",
    "    # Remove unused column\n",
    "    train_dataset = train_dataset.remove_columns(\"image\")\n",
    "    val_dataset = val_dataset.remove_columns(\"image\")\n",
    "    test_dataset = test_dataset.remove_columns(\"image\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e76cccd9-fffa-4d7c-98d8-e80e805dd106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imp = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b42126-bc4b-422a-a05b-d4efbebf63be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_processor_class': None,\n",
       " 'do_resize': True,\n",
       " 'do_rescale': True,\n",
       " 'do_normalize': True,\n",
       " 'size': {'height': 224, 'width': 224},\n",
       " 'resample': <Resampling.BILINEAR: 2>,\n",
       " 'rescale_factor': 0.00392156862745098,\n",
       " 'image_mean': [0.5, 0.5, 0.5],\n",
       " 'image_std': [0.5, 0.5, 0.5]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363a28e0-7624-468b-946d-ce6b317ce496",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f435714eb4304161a1ac7b1824374548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Apply AutoProcessor\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m train_dataset, val_dataset, test_dataset \u001B[38;5;241m=\u001B[39m apply_processing(model_name, train_dataset, val_dataset, test_dataset)\n",
      "Cell \u001B[1;32mIn[7], line 54\u001B[0m, in \u001B[0;36mapply_processing\u001B[1;34m(model_name, train_dataset, val_dataset, test_dataset)\u001B[0m\n\u001B[0;32m     51\u001B[0m image_processor \u001B[38;5;241m=\u001B[39m AutoProcessor\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# Preprocess images\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m train_dataset\u001B[38;5;241m.\u001B[39mmap(process_examples, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, features\u001B[38;5;241m=\u001B[39mfeatures, fn_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_processor\u001B[39m\u001B[38;5;124m\"\u001B[39m: image_processor}, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2500\u001B[39m)\n\u001B[0;32m     55\u001B[0m val_dataset \u001B[38;5;241m=\u001B[39m val_dataset\u001B[38;5;241m.\u001B[39mmap(process_examples, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, features\u001B[38;5;241m=\u001B[39mfeatures, fn_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_processor\u001B[39m\u001B[38;5;124m\"\u001B[39m: image_processor}, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2500\u001B[39m)\n\u001B[0;32m     56\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m test_dataset\u001B[38;5;241m.\u001B[39mmap(process_examples, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, features\u001B[38;5;241m=\u001B[39mfeatures, fn_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_processor\u001B[39m\u001B[38;5;124m\"\u001B[39m: image_processor}, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2500\u001B[39m)\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\arrow_dataset.py:2815\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   2812\u001B[0m disable_tqdm \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled()\n\u001B[0;32m   2814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m num_proc \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m-> 2815\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_map_single(\n\u001B[0;32m   2816\u001B[0m         function\u001B[38;5;241m=\u001B[39mfunction,\n\u001B[0;32m   2817\u001B[0m         with_indices\u001B[38;5;241m=\u001B[39mwith_indices,\n\u001B[0;32m   2818\u001B[0m         with_rank\u001B[38;5;241m=\u001B[39mwith_rank,\n\u001B[0;32m   2819\u001B[0m         input_columns\u001B[38;5;241m=\u001B[39minput_columns,\n\u001B[0;32m   2820\u001B[0m         batched\u001B[38;5;241m=\u001B[39mbatched,\n\u001B[0;32m   2821\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   2822\u001B[0m         drop_last_batch\u001B[38;5;241m=\u001B[39mdrop_last_batch,\n\u001B[0;32m   2823\u001B[0m         remove_columns\u001B[38;5;241m=\u001B[39mremove_columns,\n\u001B[0;32m   2824\u001B[0m         keep_in_memory\u001B[38;5;241m=\u001B[39mkeep_in_memory,\n\u001B[0;32m   2825\u001B[0m         load_from_cache_file\u001B[38;5;241m=\u001B[39mload_from_cache_file,\n\u001B[0;32m   2826\u001B[0m         cache_file_name\u001B[38;5;241m=\u001B[39mcache_file_name,\n\u001B[0;32m   2827\u001B[0m         writer_batch_size\u001B[38;5;241m=\u001B[39mwriter_batch_size,\n\u001B[0;32m   2828\u001B[0m         features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2829\u001B[0m         disable_nullable\u001B[38;5;241m=\u001B[39mdisable_nullable,\n\u001B[0;32m   2830\u001B[0m         fn_kwargs\u001B[38;5;241m=\u001B[39mfn_kwargs,\n\u001B[0;32m   2831\u001B[0m         new_fingerprint\u001B[38;5;241m=\u001B[39mnew_fingerprint,\n\u001B[0;32m   2832\u001B[0m         disable_tqdm\u001B[38;5;241m=\u001B[39mdisable_tqdm,\n\u001B[0;32m   2833\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc,\n\u001B[0;32m   2834\u001B[0m     )\n\u001B[0;32m   2835\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2837\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat_cache_file_name\u001B[39m(cache_file_name, rank):\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\arrow_dataset.py:546\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    544\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    545\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 546\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    547\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    548\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    549\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\arrow_dataset.py:513\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    506\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    508\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    509\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    510\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    511\u001B[0m }\n\u001B[0;32m    512\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 513\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    514\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    515\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\fingerprint.py:480\u001B[0m, in \u001B[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    476\u001B[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001B[0;32m    478\u001B[0m \u001B[38;5;66;03m# Call actual function\u001B[39;00m\n\u001B[1;32m--> 480\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    482\u001B[0m \u001B[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001B[39;00m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:  \u001B[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001B[39;00m\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\arrow_dataset.py:3236\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001B[0m\n\u001B[0;32m   3232\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   3233\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mslice\u001B[39m(i, i \u001B[38;5;241m+\u001B[39m batch_size)\u001B[38;5;241m.\u001B[39mindices(input_dataset\u001B[38;5;241m.\u001B[39mnum_rows)))\n\u001B[0;32m   3234\u001B[0m )  \u001B[38;5;66;03m# Something simpler?\u001B[39;00m\n\u001B[0;32m   3235\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3236\u001B[0m     batch \u001B[38;5;241m=\u001B[39m apply_function_on_filtered_inputs(\n\u001B[0;32m   3237\u001B[0m         batch,\n\u001B[0;32m   3238\u001B[0m         indices,\n\u001B[0;32m   3239\u001B[0m         check_same_num_examples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(input_dataset\u001B[38;5;241m.\u001B[39mlist_indexes()) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   3240\u001B[0m         offset\u001B[38;5;241m=\u001B[39moffset,\n\u001B[0;32m   3241\u001B[0m     )\n\u001B[0;32m   3242\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NumExamplesMismatchError:\n\u001B[0;32m   3243\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetTransformationNotAllowedError(\n\u001B[0;32m   3244\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3245\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\arrow_dataset.py:3112\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[0;32m   3110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[0;32m   3111\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[1;32m-> 3112\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m function(\u001B[38;5;241m*\u001B[39mfn_args, \u001B[38;5;241m*\u001B[39madditional_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_kwargs)\n\u001B[0;32m   3113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[0;32m   3114\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   3115\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[0;32m   3116\u001B[0m     }\n",
      "Cell \u001B[1;32mIn[7], line 23\u001B[0m, in \u001B[0;36mprocess_examples\u001B[1;34m(examples, image_processor)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Processor helper function. Used to process batches of images using the\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03mpassed image_processor.\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;124;03m    A batch of processed image examples.\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Get batch of images\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# images = examples['image']\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m images \u001B[38;5;241m=\u001B[39m [image\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mresize((\u001B[38;5;241m32\u001B[39m,\u001B[38;5;241m32\u001B[39m)) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Preprocess\u001B[39;00m\n\u001B[0;32m     26\u001B[0m inputs \u001B[38;5;241m=\u001B[39m image_processor(images\u001B[38;5;241m=\u001B[39mimages)\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\formatting\\formatting.py:282\u001B[0m, in \u001B[0;36mLazyDict.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    280\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[key]\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeys_to_format:\n\u001B[1;32m--> 282\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(key)\n\u001B[0;32m    283\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[key] \u001B[38;5;241m=\u001B[39m value\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeys_to_format\u001B[38;5;241m.\u001B[39mremove(key)\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\formatting\\formatting.py:385\u001B[0m, in \u001B[0;36mLazyBatch.format\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    384\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[1;32m--> 385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformatter\u001B[38;5;241m.\u001B[39mformat_column(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpa_table\u001B[38;5;241m.\u001B[39mselect([key]))\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\formatting\\formatting.py:447\u001B[0m, in \u001B[0;36mPythonFormatter.format_column\u001B[1;34m(self, pa_table)\u001B[0m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, pa_table: pa\u001B[38;5;241m.\u001B[39mTable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m    446\u001B[0m     column \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_arrow_extractor()\u001B[38;5;241m.\u001B[39mextract_column(pa_table)\n\u001B[1;32m--> 447\u001B[0m     column \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_features_decoder\u001B[38;5;241m.\u001B[39mdecode_column(column, pa_table\u001B[38;5;241m.\u001B[39mcolumn_names[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m    448\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m column\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\formatting\\formatting.py:228\u001B[0m, in \u001B[0;36mPythonFeaturesDecoder.decode_column\u001B[1;34m(self, column, column_name)\u001B[0m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, column: \u001B[38;5;28mlist\u001B[39m, column_name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m--> 228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures\u001B[38;5;241m.\u001B[39mdecode_column(column, column_name) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures \u001B[38;5;28;01melse\u001B[39;00m column\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\features\\features.py:1868\u001B[0m, in \u001B[0;36mFeatures.decode_column\u001B[1;34m(self, column, column_name)\u001B[0m\n\u001B[0;32m   1855\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, column: \u001B[38;5;28mlist\u001B[39m, column_name: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1856\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Decode column with custom feature decoding.\u001B[39;00m\n\u001B[0;32m   1857\u001B[0m \n\u001B[0;32m   1858\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;124;03m        `list[Any]`\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m-> 1868\u001B[0m         [decode_nested_example(\u001B[38;5;28mself\u001B[39m[column_name], value) \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m value \u001B[38;5;129;01min\u001B[39;00m column]\n\u001B[0;32m   1869\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_column_requires_decoding[column_name]\n\u001B[0;32m   1870\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m column\n\u001B[0;32m   1871\u001B[0m     )\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\features\\features.py:1868\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1855\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, column: \u001B[38;5;28mlist\u001B[39m, column_name: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1856\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Decode column with custom feature decoding.\u001B[39;00m\n\u001B[0;32m   1857\u001B[0m \n\u001B[0;32m   1858\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;124;03m        `list[Any]`\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m-> 1868\u001B[0m         [decode_nested_example(\u001B[38;5;28mself\u001B[39m[column_name], value) \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m value \u001B[38;5;129;01min\u001B[39;00m column]\n\u001B[0;32m   1869\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_column_requires_decoding[column_name]\n\u001B[0;32m   1870\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m column\n\u001B[0;32m   1871\u001B[0m     )\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\features\\features.py:1309\u001B[0m, in \u001B[0;36mdecode_nested_example\u001B[1;34m(schema, obj, token_per_repo_id)\u001B[0m\n\u001B[0;32m   1306\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (Audio, Image)):\n\u001B[0;32m   1307\u001B[0m     \u001B[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001B[39;00m\n\u001B[0;32m   1308\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m schema\u001B[38;5;241m.\u001B[39mdecode:\n\u001B[1;32m-> 1309\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m schema\u001B[38;5;241m.\u001B[39mdecode_example(obj, token_per_repo_id\u001B[38;5;241m=\u001B[39mtoken_per_repo_id)\n\u001B[0;32m   1310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\datasets\\features\\image.py:176\u001B[0m, in \u001B[0;36mImage.decode_example\u001B[1;34m(self, value, token_per_repo_id)\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     image \u001B[38;5;241m=\u001B[39m PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mopen(BytesIO(bytes_))\n\u001B[1;32m--> 176\u001B[0m image\u001B[38;5;241m.\u001B[39mload()  \u001B[38;5;66;03m# to avoid \"Too many open files\" errors\u001B[39;00m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image\n",
      "File \u001B[1;32mC:\\anaconda-dev-stand\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001B[0m, in \u001B[0;36mImageFile.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[0;32m    268\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[1;32m--> 269\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m decoder\u001B[38;5;241m.\u001B[39mdecode(b)\n\u001B[0;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    271\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Apply AutoProcessor\n",
    "train_dataset, val_dataset, test_dataset = apply_processing(model_name, train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b38c68-40b3-4185-abaa-43f9359dd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_path = './data/processed_dataset_32x32/train/'\n",
    "val_save_path = './data/processed_dataset_32x32/val/'\n",
    "test_save_path = './data/processed_dataset_32x32/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bd5ea79-4395-4d56-85b5-a7a3388066c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ea174a30b7449bab43c2311a2c87e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/42 shards):   0%|          | 0/34421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270536078d8347a3b6ba35d354749675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/12 shards):   0%|          | 0/9839 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2289991a354ae5b437c0ef9dad23a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/6 shards):   0%|          | 0/4913 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk(train_save_path)\n",
    "val_dataset.save_to_disk(val_save_path)\n",
    "test_dataset.save_to_disk(test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4f293-e332-45fe-8305-212ebdda1e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
